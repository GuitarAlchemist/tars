{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft": "Warning",
      "System": "Warning"
    },
    "NLog": {
      "IncludeScopes": true
    }
  },
  "Ollama": {
    "BaseUrl": "http://localhost:11434",
    "DefaultModel": "codellama:13b-code",
    "RequiredModels": "llama3,all-minilm"
  },
  "Tars": {
    "ProjectRoot": "C:\\Users\\spare\\source\\repos\\tars",
    "LogLevel": "Information",
    "Mcp": {
      "Port": 8999,
      "AutoExecuteEnabled": true,
      "AutoExecuteCommands": true,
      "AutoCodeGeneration": true
    },
    "A2A": {
      "Host": "localhost",
      "Port": 8998,
      "Enabled": true
    }
  },
  "HuggingFace": {
    "ApiKey": "",
    "DefaultTask": "text-generation"
  },
  "DockerModelRunner": {
    "BaseUrl": "http://localhost:8080",
    "DefaultModel": "llama3:8b",
    "RecommendedModels": [
      "llama3:8b-instruct",
      "llama3:70b-instruct",
      "claude-3-opus-20240229",
      "claude-3-sonnet-20240229",
      "gpt-4o",
      "gemini-1.5-pro"
    ]
  },
  "ModelProvider": {
    "Default": "Ollama"
  }
}
