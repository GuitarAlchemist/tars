CONFIG {
    model: "llama3"
    temperature: 0.7
    max_tokens: 1000
}

PROMPT {
    text: "What is the capital of France?"
    model: "llama3"
}

ACTION {
    type: "generate"
    model: "llama3"
}
